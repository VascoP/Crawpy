{"name":"Crawpy","body":"# crawpy\r\nAn attempt at a python web crawler\r\n\r\n### Usage\r\n\r\nCreate a mysql table. If you don't know how to do this I recommend using phpmyadmin. Collation should be `utf8_general_ci`.\r\n\r\nCustomize `settings.py` with your database *settings*.\r\n\r\nNow you can add links to the `link` table directly or you can edit the `SEED_LINKS` list in `settings.py` and run `python explorer.py --seed`\r\n\r\nIf you're in a hurry you can leave `SEED_LINKS` alone and just seed from the command line with `python explorer.py -seed http://myfirstseed.com http://mysecondseed.net ...`\r\n\r\nThe seeding procedure is only required if you have no uncrawled links in your database (ie: first time running the crawler).\r\n\r\nAfter getting a few pages, start a `harvester.py`.\r\nYou can run as many explorers and harvesters concurrently as you like.\r\n\r\n### How it works\r\nEach explorer gets uncrawled links from the `link` table, retrieves their html contents and dumps them into the `webpage` table.\r\nThe harvester gets an unharvested html content from the `webpage` table, extracts links, and stores them in as uncrawled links in the `link` table.\r\n\r\n![workflow](http://i.imgur.com/QV05o.jpg)\r\n\r\n### Todo\r\n\r\n* Automate explorer/harvester relation\r\n* Better error handling\r\n","tagline":"An attempt at a python web crawler","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}